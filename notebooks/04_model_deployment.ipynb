{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# =============================================================\n",
    "# MILESTONE 4: MLOps, Deployment, and Monitoring\n",
    "# ============================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import shap\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from scipy.stats import ks_2samp\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PEACH      = '#FFCBA4'\n",
    "PEACH_DARK = '#FF9A76'\n",
    "SAGE       = '#A8C686'\n",
    "SAGE_DARK  = '#7A9B57'\n",
    "NEUTRAL    = '#F5F5DC'\n",
    "ACCENT     = '#E07B39'\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 7)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = Path.cwd().parent if 'notebooks' in str(Path.cwd()) else Path.cwd()\n",
    "MODELS_DIR   = PROJECT_ROOT / \"models\" / \"trained_models\"\n",
    "ARTIFACTS_DIR = PROJECT_ROOT / \"models\" / \"artifacts\"\n",
    "DATA_PATH    = PROJECT_ROOT / \"data\" / \"processed\" / \"final_processed_data.csv\"\n",
    "VIZ_STATIC   = PROJECT_ROOT / \"visualizations\" / \"static\" \n",
    "VIZ_INTER    = PROJECT_ROOT / \"visualizations\" / \"interactive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[1/8] Loading production artifacts...\")\n",
    "best_model = joblib.load(MODELS_DIR / \"best_churn_model.pkl\")\n",
    "selected_features = joblib.load(ARTIFACTS_DIR / \"selected_features.pkl\")\n",
    "X_test, y_test = joblib.load(ARTIFACTS_DIR / \"test_data.pkl\")\n",
    "\n",
    "df_full = pd.read_csv(DATA_PATH)\n",
    "X_full = df_full.drop('Churn', axis=1)\n",
    "y_full = df_full['Churn'].astype(int)\n",
    "\n",
    "print(f\"Model: {type(best_model).__name__}\")\n",
    "print(f\"Features: {len(selected_features)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[2/8] Computing SHAP values...\")\n",
    "explainer = shap.TreeExplainer(best_model)\n",
    "X_sample = X_test.sample(n=min(500, len(X_test)), random_state=42)\n",
    "\n",
    "shap_values = explainer.shap_values(X_sample)\n",
    "\n",
    "# ---- FIX: Ensure shap_vals is always (samples, features) ----\n",
    "if isinstance(shap_values, list):\n",
    "    shap_vals = shap_values[1]\n",
    "elif shap_values.ndim == 3:\n",
    "    shap_vals = shap_values[1]\n",
    "else:\n",
    "    shap_vals = shap_values\n",
    "\n",
    "print(\"shap_vals shape:\", shap_vals.shape)\n",
    "\n",
    "# Align feature names if needed\n",
    "if len(selected_features) != shap_vals.shape[1]:\n",
    "    print(\"WARNING: feature count mismatch → fixing automatically\")\n",
    "    selected_features = selected_features[:shap_vals.shape[1]]\n",
    "\n",
    "# Global Feature Importance\n",
    "shap_importance = pd.DataFrame({\n",
    "    'Feature': selected_features,\n",
    "    'Mean_|SHAP|': np.abs(shap_vals).mean(axis=0)\n",
    "}).sort_values('Mean_|SHAP|', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[3/8] Testing FastAPI endpoint...\")\n",
    "API_URL = \"http://127.0.0.1:8000/predict\"\n",
    "\n",
    "sample_payload = {\n",
    "    \"Account length\": 50,\n",
    "    \"International plan\": \"Yes\",\n",
    "    \"Voice mail plan\": \"No\",\n",
    "    \"Number vmail messages\": 0,\n",
    "    \"Total day minutes\": 300.0,\n",
    "    \"Customer service calls\": 5\n",
    "}\n",
    "\n",
    "try:\n",
    "    resp = requests.post(API_URL, json=sample_payload, timeout=10)\n",
    "    if resp.status_code == 200:\n",
    "        result = resp.json()\n",
    "        print(\"API CONNECTED & WORKING\")\n",
    "        print(f\"→ Churn Probability: {result['churn_probability']:.2%}\")\n",
    "        print(f\"→ Top Driver: {result['top_shap_features'][0]['feature']} ({result['top_shap_features'][0]['shap_value']:+.3f})\")\n",
    "    else:\n",
    "        print(f\"API Error {resp.status_code}: {resp.text}\")\n",
    "except Exception as e:\n",
    "    print(\"API not reachable. Start with: uvicorn src.api.main:app --reload\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[4/8] Performance benchmarking...\")\n",
    "sizes = [1, 10, 50, 100, 500]\n",
    "bench = []\n",
    "for size in sizes:\n",
    "    batch = X_test.sample(n=min(size, len(X_test)), random_state=42)\n",
    "    start = time.time()\n",
    "    _ = best_model.predict_proba(batch)\n",
    "    elapsed = (time.time() - start) * 1000\n",
    "    bench.append({'Size': size, 'Latency_ms': elapsed, 'Per_Sample_ms': elapsed/size})\n",
    "\n",
    "bench_df = pd.DataFrame(bench)\n",
    "fig = make_subplots(1, 2, subplot_titles=(\"Latency per Sample\", \"Throughput\"))\n",
    "fig.add_trace(go.Scatter(x=bench_df['Size'], y=bench_df['Per_Sample_ms'], mode='lines+markers',\n",
    "                         name='Latency/Sample', line=dict(color=PEACH_DARK)), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=bench_df['Size'], y=bench_df['Size']/(bench_df['Latency_ms']/1000),\n",
    "                         mode='lines+markers', name='Throughput', line=dict(color=SAGE_DARK)), row=1, col=2)\n",
    "fig.update_layout(title=\"Model Inference Performance\", height=400, template=\"plotly_white\")\n",
    "fig.write_html(VIZ_INTER / \"04_latency_throughput.html\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[5/8] Data drift analysis...\")\n",
    "train_split = int(0.8 * len(X_full))\n",
    "X_train_ref = X_full.iloc[:train_split]\n",
    "X_prod = X_full.iloc[train_split:]\n",
    "\n",
    "drift_results = []\n",
    "for feat in selected_features:\n",
    "    stat, p = ks_2samp(X_train_ref[feat], X_prod[feat])\n",
    "    drift_results.append({'Feature': feat, 'KS': stat, 'p_value': p, 'Drift': p < 0.05})\n",
    "\n",
    "drift_df = pd.DataFrame(drift_results).sort_values('KS', ascending=False)\n",
    "drifted_count = drift_df['Drift'].sum()\n",
    "print(f\"{drifted_count}/{len(selected_features)} features show significant drift\")\n",
    "\n",
    "fig = px.bar(drift_df.head(15), x='KS', y='Feature', orientation='h',\n",
    "             color='Drift', color_discrete_map={True: PEACH_DARK, False: SAGE_DARK},\n",
    "             title=\"Top 15 Features by Data Drift (KS Test)\")\n",
    "fig.update_layout(yaxis=dict(autorange=\"reversed\"), height=600)\n",
    "fig.write_html(VIZ_INTER / \"05_data_drift.html\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[6/8] Business impact analysis...\")\n",
    "proba = best_model.predict_proba(X_test)[:, 1]\n",
    "risk_df = pd.DataFrame({'prob': proba, 'actual': y_test.values})\n",
    "risk_df['segment'] = pd.cut(proba, [0, 0.3, 0.7, 1.0], labels=['Low', 'Medium', 'High'])\n",
    "\n",
    "segment_stats = risk_df.groupby('segment').agg(\n",
    "    count=('prob', 'size'),\n",
    "    avg_prob=('prob', 'mean'),\n",
    "    churn_rate=('actual', 'mean')\n",
    ").round(4)\n",
    "\n",
    "high_risk_n = segment_stats.loc['High', 'count']\n",
    "revenue_at_risk = high_risk_n * 1200  # $1200 annual value\n",
    "campaign_cost = high_risk_n * 200\n",
    "success_rate = 0.40\n",
    "retained_value = high_risk_n * success_rate * 1200\n",
    "roi = (retained_value - campaign_cost) / campaign_cost * 100\n",
    "\n",
    "fig = px.histogram(risk_df, x='prob', color='segment',\n",
    "                   color_discrete_map={'Low': SAGE_DARK, 'Medium': PEACH, 'High': PEACH_DARK},\n",
    "                   title=\"Customer Distribution by Predicted Churn Risk\")\n",
    "fig.write_html(VIZ_INTER / \"06_risk_segments.html\")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
