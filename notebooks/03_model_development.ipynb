{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# =============================================================\n",
    "# MILESTONE 3: Machine Learning Model Development and Optimization\n",
    "# ============================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Train All Models\n",
    "# python ../src/modeling/train_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  2. Evaluate All Models\n",
    "# python ../src/modeling/evaluate.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, precision_recall_curve\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "MODEL_DIR = Path(\"../models/trained_models\")\n",
    "STATIC_VIZ = Path(\"../visualizations/static\")\n",
    "INTER_VIZ = Path(\"../visualizations/interactive\")\n",
    "STATIC_VIZ.mkdir(parents=True, exist_ok=True)\n",
    "INTER_VIZ.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load results\n",
    "with open(MODEL_DIR / \"evaluation_results.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "results = data[\"all_results\"]\n",
    "best_model = data[\"best_model\"]\n",
    "\n",
    "# Convert to DataFrame\n",
    "metrics_df = pd.DataFrame([\n",
    "    {k: v for k, v in r.items() if k not in [\"y_true\", \"y_pred\", \"y_prob\"]}\n",
    "    for r in results\n",
    "])\n",
    "metrics_df = metrics_df.round(4)\n",
    "print(\"MODEL COMPARISON\")\n",
    "display(metrics_df)\n",
    "\n",
    "print(f\"\\nBEST MODEL: {best_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, r in enumerate(results):\n",
    "    cm = confusion_matrix(r[\"y_true\"], r[\"y_pred\"])\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i],\n",
    "                xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])\n",
    "    axes[i].set_title(f\"{r['model']}\")\n",
    "    axes[i].set_ylabel(\"True\")\n",
    "    axes[i].set_xlabel(\"Predicted\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(STATIC_VIZ / \"confusion_matrices.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=(\"ROC Curves\", \"Precision-Recall Curves\"),\n",
    "    horizontal_spacing=0.15\n",
    ")\n",
    "\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "for i, r in enumerate(results):\n",
    "    # ROC\n",
    "    fpr, tpr, _ = roc_curve(r[\"y_true\"], r[\"y_prob\"])\n",
    "    fig.add_trace(go.Scatter(x=fpr, y=tpr, mode='lines',\n",
    "                             name=f\"{r['model']} (AUC={r['roc_auc']:.3f})\",\n",
    "                             line=dict(color=colors[i])), row=1, col=1)\n",
    "    \n",
    "    # PR\n",
    "    precision, recall, _ = precision_recall_curve(r[\"y_true\"], r[\"y_prob\"])\n",
    "    fig.add_trace(go.Scatter(x=recall, y=precision, mode='lines',\n",
    "                             name=r['model'], line=dict(color=colors[i]), showlegend=False),\n",
    "                  row=1, col=2)\n",
    "\n",
    "# Diagonal line for ROC\n",
    "fig.add_trace(go.Scatter(x=[0,1], y=[0,1], mode='lines',\n",
    "                         line=dict(color='gray', dash='dash'), showlegend=False), row=1, col=1)\n",
    "\n",
    "fig.update_layout(height=550, width=1100, template=\"plotly_white\",\n",
    "                  title_text=\"Model Comparison: ROC & PR Curves\", title_x=0.5)\n",
    "fig.update_xaxes(title=\"False Positive Rate\", row=1, col=1)\n",
    "fig.update_yaxes(title=\"True Positive Rate\", row=1, col=1)\n",
    "fig.update_xaxes(title=\"Recall\", row=1, col=2)\n",
    "fig.update_yaxes(title=\"Precision\", row=1, col=2)\n",
    "\n",
    "curve_path = INTER_VIZ / \"model_comparison_curves.html\"\n",
    "fig.write_html(curve_path)\n",
    "print(f\"Interactive curves saved â†’ {curve_path}\")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
