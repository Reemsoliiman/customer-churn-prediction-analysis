{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# =============================================================\n",
    "# MILESTONE 3: Machine Learning Model Development and Optimization\n",
    "# ============================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    confusion_matrix, classification_report, roc_curve, precision_recall_curve\n",
    ")\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PEACH      = '#FFCBA4'\n",
    "PEACH_DARK = '#FF9A76'\n",
    "SAGE       = '#A8C686'\n",
    "SAGE_DARK  = '#7A9B57'\n",
    "NEUTRAL    = '#F5F5DC'\n",
    "ACCENT     = '#E07B39'\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = Path.cwd().parent if 'notebooks' in str(Path.cwd()) else Path.cwd()\n",
    "DATA_PATH = PROJECT_ROOT / \"data\" / \"processed\" / \"final_processed_data.csv\"\n",
    "STATIC_VIZ = PROJECT_ROOT / \"visualizations\" / \"static\" \n",
    "INTER_VIZ  = PROJECT_ROOT / \"visualizations\" / \"interactive\" \n",
    "MODELS_DIR = PROJECT_ROOT / \"models\" / \"trained_models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH)\n",
    "X = df.drop('Churn', axis=1)\n",
    "y = df['Churn'].astype(int)\n",
    "print(f\"Dataset: {df.shape[0]:,} samples × {df.shape[1]-1} features | Churn rate: {y.mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print(f\"Train: {X_train.shape[0]} | Test: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[2/9] Training baseline models...\")\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1),\n",
    "    'XGBoost': XGBClassifier(n_estimators=200, eval_metric='logloss', random_state=42, n_jobs=-1)\n",
    "}\n",
    "\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    print(f\"  → {name}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    results.append({\n",
    "        'model_name': name,\n",
    "        'model_obj': model,\n",
    "        'y_pred': y_pred,\n",
    "        'y_proba': y_proba,\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred),\n",
    "        'auc': roc_auc_score(y_test, y_proba)\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame([{\n",
    "    'Model': r['model_name'],\n",
    "    'Accuracy': r['accuracy'],\n",
    "    'Precision': r['precision'],\n",
    "    'Recall': r['recall'],\n",
    "    'F1': r['f1'],\n",
    "    'ROC-AUC': r['auc']\n",
    "} for r in results])\n",
    "\n",
    "print(\"\\nBaseline Results:\")\n",
    "print(results_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[3/9] Hyperparameter tuning...\")\n",
    "\n",
    "# XGBoost Tuning\n",
    "print(\"  → Tuning XGBoost...\")\n",
    "xgb_grid = GridSearchCV(\n",
    "    XGBClassifier(eval_metric='logloss', random_state=42),\n",
    "    param_grid={\n",
    "        'n_estimators': [200, 300],\n",
    "        'mav_depth': [4, 6],\n",
    "        'learning_rate': [0.05, 0.1],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'colsample_bytree': [0.8, 1.0]\n",
    "    },\n",
    "    scoring='roc_auc', cv=3, n_jobs=-1, verbose=0\n",
    ")\n",
    "xgb_grid.fit(X_train, y_train)\n",
    "best_xgb = xgb_grid.best_estimator_\n",
    "print(f\"     Best AUC: {xgb_grid.best_score_:.4f}\")\n",
    "\n",
    "# RF Tuning\n",
    "print(\"  → Tuning Random Forest...\")\n",
    "rf_grid = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "    param_grid={\n",
    "        'n_estimators': [300, 500],\n",
    "        'max_depth': [None, 20],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'min_samples_leaf': [1, 2]\n",
    "    },\n",
    "    scoring='roc_auc', cv=3, n_jobs=-1, verbose=0\n",
    ")\n",
    "rf_grid.fit(X_train, y_train)\n",
    "best_rf = rf_grid.best_estimator_\n",
    "\n",
    "# Retrain best models\n",
    "best_xgb.fit(X_train, y_train)\n",
    "best_rf.fit(X_train, y_train)\n",
    "\n",
    "# Final predictions\n",
    "for name, model in [('XGBoost (Tuned)', best_xgb), ('Random Forest (Tuned)', best_rf)]:\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    results.append({\n",
    "        'model_name': name,\n",
    "        'model_obj': model,\n",
    "        'y_pred': y_pred,\n",
    "        'y_proba': y_proba,\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred),\n",
    "        'auc': roc_auc_score(y_test, y_proba)\n",
    "    })\n",
    "\n",
    "final_df = pd.DataFrame([{\n",
    "    'Model': r['model_name'],\n",
    "    'Accuracy': r['accuracy'],\n",
    "    'Precision': r['precision'],\n",
    "    'Recall': r['recall'],\n",
    "    'F1': r['f1'],\n",
    "    'ROC-AUC': r['auc']\n",
    "} for r in results]).round(4)\n",
    "\n",
    "print(\"\\nFinal Model Comparison:\")\n",
    "print(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_idx = final_df['ROC-AUC'].idxmax()\n",
    "best_result = results[best_idx]\n",
    "best_model = best_result['model_obj']\n",
    "best_name = best_result['model_name']\n",
    "\n",
    "print(f\"\\nBEST MODEL: {best_name} | ROC-AUC: {best_result['auc']:.4f}\")\n",
    "\n",
    "# Save best agent\n",
    "joblib.dump(best_model, MODELS_DIR / \"best_churn_model.pkl\")\n",
    "print(f\"Model saved → {MODELS_DIR}/best_churn_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, best_result['y_pred'])\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "# Interactive (Plotly)\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=cm, x=['No Churn', 'Churn'], y=['No Churn', 'Churn'],\n",
    "    colorscale=[[0, SAGE_DARK], [0.5, NEUTRAL], [1, PEACH_DARK]],\n",
    "    text=cm, texttemplate=\"%{text}\", textfont={\"size\": 20}\n",
    "))\n",
    "fig.update_layout(title=f'Confusion Matrix - {best_name}', height=500)\n",
    "fig.write_html(INTER_VIZ / \"01_confusion_matrix.html\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"ROC Curve\", \"Precision-Recall Curve\"))\n",
    "\n",
    "colors = [SAGE_DARK, PEACH_DARK, ACCENT]\n",
    "for i, r in enumerate(results[-3:]):  # Top 3 models\n",
    "    # ROC\n",
    "    fpr, tpr, _ = roc_curve(y_test, r['y_proba'])\n",
    "    fig.add_trace(go.Scatter(x=fpr, y=tpr, mode='lines',\n",
    "                             name=f\"{r['model_name']} (AUC={r['auc']:.3f})\",\n",
    "                             line=dict(color=colors[i])), row=1, col=1)\n",
    "    # PR\n",
    "    p, r_curve, _ = precision_recall_curve(y_test, r['y_proba'])\n",
    "    fig.add_trace(go.Scatter(x=r_curve, y=p, mode='lines',\n",
    "                             name=r['model_name'], line=dict(color=colors[i]),\n",
    "                             showlegend=False), row=1, col=2)\n",
    "\n",
    "# Reference lines\n",
    "fig.add_trace(go.Scatter(x=[0,1], y=[0,1], line=dict(dash='dash', color='gray'),\n",
    "                         showlegend=False), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=[0,1], y=[y_test.mean()]*2, line=dict(dash='dash', color='gray'),\n",
    "                         showlegend=False), row=1, col=2)\n",
    "\n",
    "fig.update_layout(height=550, title_text=\"Model Comparison: ROC & Precision-Recall Curves\", template=\"plotly_white\")\n",
    "fig.write_html(INTER_VIZ / \"02_roc_pr_curves.html\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    imp = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': best_model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False).head(15)\n",
    "\n",
    "    # Interactive\n",
    "    fig = px.bar(imp, x='Importance', y='Feature', orientation='h',\n",
    "                 color='Importance', color_continuous_scale='Peach')\n",
    "    fig.update_layout(title=f'Top 15 Features - {best_name}', height=600)\n",
    "    fig.write_html(INTER_VIZ / \"03_feature_importance.html\")\n",
    "    fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
